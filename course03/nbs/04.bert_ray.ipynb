{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text articles: 777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from transformers import pipeline\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import psutil\n",
    "import ray\n",
    "import os\n",
    "# os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = \"python\" \n",
    "# Load 20newsgroups news articles that that belong to “rec.motorcycles” and “rec.sport.baseball” classes of the test set only\n",
    "test_data = fetch_20newsgroups(subset='test', shuffle=False, categories=['rec.motorcycles', 'rec.sport.baseball'], remove=('headers', 'footers', 'quotes'))\n",
    "# Remove empty news article texts\n",
    "test_data = [text for text in test_data.data if text!='']\n",
    "print('Number of text articles:', len(test_data))\n",
    "\n",
    "\"\"\"\n",
    "HuggingFace pipelines are objects that abstract most of the complex code from the library, \n",
    "offering a simple API dedicated to several tasks, including text classification.\n",
    "All pipelines can use batching.\n",
    "However, this is not automatically a win for performance. \n",
    "It can be either a 10x speedup or 5x slowdown depending on hardware, data and the actual model being used.\n",
    "Batching is only recommended on GPU. \n",
    "If you are using CPU, don’t batch.\n",
    "\"\"\"\n",
    "# Init pipeline with batchsize 1 on CPU for our example\n",
    "pipe = pipeline(task = 'zero-shot-classification', model='typeform/distilbert-base-uncased-mnli', batch_size=1, device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/devEnv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequires\n",
    "```\n",
    "pip install ray\n",
    "pip install torch\n",
    "pip install transformers\n",
    "pip install scikit-learn\n",
    "pip install psutil\n",
    "```\n",
    "https://towardsdatascience.com/parallel-inference-of-huggingface-transformers-on-cpus-4487c28abe23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict single text\n",
    "prediction = pipe(test_data[100], ['motorcycle', 'baseball'])\n",
    "print('Text:', prediction['sequence'])\n",
    "print('Labels:', prediction['labels'])\n",
    "print('Scores:', prediction['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hey, the Lone Biker of the Apocalypse (see Raising Arizona) had flames coming\n",
      "out of both his exhaust pipes. I love to toggle the kill switch on my Sportster\n",
      "to produce flaming backfires, especially underneath overpasses at night (it's\n",
      "loud and lights up the whole underpass!!!\n",
      "Labels: ['motorcycle', 'baseball']\n",
      "Scores: [0.9970590472221375, 0.0029409313574433327]\n"
     ]
    }
   ],
   "source": [
    "# Predict single text\n",
    "prediction = pipe(test_data[100], ['motorcycle', 'baseball'])\n",
    "print('Text:', prediction['sequence'])\n",
    "print('Labels:', prediction['labels'])\n",
    "print('Scores:', prediction['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time: 0:00:38.676117\n"
     ]
    }
   ],
   "source": [
    "# Predict multipe texts on single CPU and time the inference duration\n",
    "start = time.time()\n",
    "predictions = [pipe(text, ['motorcycle', 'baseball']) for text in test_data]\n",
    "end = time.time()\n",
    "print('Prediction time:', str(timedelta(seconds=end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPUs: 32\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 15:04:16,157\tINFO worker.py:1528 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Prediction time: 0:01:14.178905\n"
     ]
    }
   ],
   "source": [
    "num_cpus = psutil.cpu_count(logical=True)\n",
    "print('Number of available CPUs:', num_cpus)\n",
    "\n",
    "# Start Ray cluster\n",
    "ray.init(num_cpus=num_cpus, ignore_reinit_error=True)\n",
    "\n",
    "\"\"\"\n",
    "The command ray.put(x) would be run by a worker process or by the driver process (the driver process is the one running your script). \n",
    "It takes a Python object and copies it to the local object store (here local means on the same node). \n",
    "Once the object has been stored in the object store, its value cannot be changed.\n",
    "In addition, ray.put(x) returns an object ID, which is essentially an ID that can be used to refer to the newly created remote object. \n",
    "If we save the object ID in a variable with x_id = ray.put(x), \n",
    "then we can pass x_id into remote functions, \n",
    "and those remote functions will operate on the corresponding remote object.\n",
    "\"\"\"\n",
    "pipe_id = ray.put(pipe)\n",
    "\n",
    "# @ray.remote decorator enables to use this \n",
    "# function in distributed setting\n",
    "@ray.remote\n",
    "def predict(pipeline, text_data, label_names):\n",
    "    return pipeline(text_data, label_names)\n",
    "\n",
    "# Predict multipe texts on all available CPUs and time the inference duration\n",
    "start = time.time()\n",
    "\n",
    "# Run the function using multiple cores and gather the results\n",
    "predictions = ray.get([predict.remote(pipe_id, text, ['motorcycle', 'baseball']) for text in test_data])\n",
    "\n",
    "end = time.time()\n",
    "print('Prediction time:', str(timedelta(seconds=end-start)))\n",
    "\n",
    "# Stop running Ray cluster\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devEnv",
   "language": "python",
   "name": "devenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
