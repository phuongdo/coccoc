{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequires\n",
    "You can install the scikit-learn library using the pip Python installer, as follows:\n",
    "\n",
    "```$sudo pip install scikit-learn matplotlib```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check scikit-learn version\n",
    "import sklearn\n",
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Common Dimensionality Reduction Techniques\n",
    "\n",
    "We use `make_classification()` function to create a test binary classification dataset.\n",
    "The dataset will have ```1,000 examples``` with ```20 input features```, 10 of which are informative and 10 of which are redundant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.080548</td>\n",
       "      <td>0.822733</td>\n",
       "      <td>-1.211753</td>\n",
       "      <td>2.421184</td>\n",
       "      <td>3.304243</td>\n",
       "      <td>-6.343576</td>\n",
       "      <td>-0.369886</td>\n",
       "      <td>0.064277</td>\n",
       "      <td>0.094521</td>\n",
       "      <td>-4.364430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502341</td>\n",
       "      <td>-2.339147</td>\n",
       "      <td>0.153031</td>\n",
       "      <td>0.543728</td>\n",
       "      <td>-0.420523</td>\n",
       "      <td>1.456127</td>\n",
       "      <td>2.882609</td>\n",
       "      <td>1.791600</td>\n",
       "      <td>-4.297088</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.330300</td>\n",
       "      <td>-4.866086</td>\n",
       "      <td>-3.882913</td>\n",
       "      <td>-2.232483</td>\n",
       "      <td>1.445153</td>\n",
       "      <td>2.597391</td>\n",
       "      <td>3.689269</td>\n",
       "      <td>-1.651189</td>\n",
       "      <td>-2.478660</td>\n",
       "      <td>-1.719449</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.885896</td>\n",
       "      <td>-1.440399</td>\n",
       "      <td>3.128698</td>\n",
       "      <td>-5.370488</td>\n",
       "      <td>3.881865</td>\n",
       "      <td>0.759844</td>\n",
       "      <td>-0.145616</td>\n",
       "      <td>-0.554894</td>\n",
       "      <td>0.614208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.197150</td>\n",
       "      <td>1.555631</td>\n",
       "      <td>-0.618716</td>\n",
       "      <td>7.193674</td>\n",
       "      <td>-3.300375</td>\n",
       "      <td>-0.033224</td>\n",
       "      <td>4.182462</td>\n",
       "      <td>0.290963</td>\n",
       "      <td>0.886022</td>\n",
       "      <td>-0.685233</td>\n",
       "      <td>...</td>\n",
       "      <td>1.015341</td>\n",
       "      <td>2.441568</td>\n",
       "      <td>0.932073</td>\n",
       "      <td>-1.899996</td>\n",
       "      <td>-3.049732</td>\n",
       "      <td>-3.174851</td>\n",
       "      <td>1.734818</td>\n",
       "      <td>0.130674</td>\n",
       "      <td>-3.133515</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.535769</td>\n",
       "      <td>-0.733499</td>\n",
       "      <td>0.204541</td>\n",
       "      <td>0.907992</td>\n",
       "      <td>-1.142800</td>\n",
       "      <td>-2.342064</td>\n",
       "      <td>2.533897</td>\n",
       "      <td>-1.140052</td>\n",
       "      <td>4.231472</td>\n",
       "      <td>0.032415</td>\n",
       "      <td>...</td>\n",
       "      <td>2.093248</td>\n",
       "      <td>-2.800467</td>\n",
       "      <td>-2.093340</td>\n",
       "      <td>1.102820</td>\n",
       "      <td>1.385990</td>\n",
       "      <td>-0.476395</td>\n",
       "      <td>3.055135</td>\n",
       "      <td>1.764456</td>\n",
       "      <td>-1.132424</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.947908</td>\n",
       "      <td>3.409123</td>\n",
       "      <td>3.574408</td>\n",
       "      <td>-4.579258</td>\n",
       "      <td>3.181229</td>\n",
       "      <td>-2.982798</td>\n",
       "      <td>-3.618861</td>\n",
       "      <td>1.490834</td>\n",
       "      <td>3.125231</td>\n",
       "      <td>-1.153063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.668844</td>\n",
       "      <td>-3.032168</td>\n",
       "      <td>-2.448262</td>\n",
       "      <td>2.317296</td>\n",
       "      <td>5.087334</td>\n",
       "      <td>1.568465</td>\n",
       "      <td>-0.429839</td>\n",
       "      <td>1.992009</td>\n",
       "      <td>2.669929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.080548  0.822733 -1.211753  2.421184  3.304243 -6.343576 -0.369886   \n",
       "1 -2.330300 -4.866086 -3.882913 -2.232483  1.445153  2.597391  3.689269   \n",
       "2 -1.197150  1.555631 -0.618716  7.193674 -3.300375 -0.033224  4.182462   \n",
       "3  1.535769 -0.733499  0.204541  0.907992 -1.142800 -2.342064  2.533897   \n",
       "4  1.947908  3.409123  3.574408 -4.579258  3.181229 -2.982798 -3.618861   \n",
       "\n",
       "          7         8         9  ...        11        12        13        14  \\\n",
       "0  0.064277  0.094521 -4.364430  ...  0.502341 -2.339147  0.153031  0.543728   \n",
       "1 -1.651189 -2.478660 -1.719449  ... -3.885896 -1.440399  3.128698 -5.370488   \n",
       "2  0.290963  0.886022 -0.685233  ...  1.015341  2.441568  0.932073 -1.899996   \n",
       "3 -1.140052  4.231472  0.032415  ...  2.093248 -2.800467 -2.093340  1.102820   \n",
       "4  1.490834  3.125231 -1.153063  ...  0.668844 -3.032168 -2.448262  2.317296   \n",
       "\n",
       "         15        16        17        18        19  class  \n",
       "0 -0.420523  1.456127  2.882609  1.791600 -4.297088      0  \n",
       "1  3.881865  0.759844 -0.145616 -0.554894  0.614208      0  \n",
       "2 -3.049732 -3.174851  1.734818  0.130674 -3.133515      0  \n",
       "3  1.385990 -0.476395  3.055135  1.764456 -1.132424      1  \n",
       "4  5.087334  1.568465 -0.429839  1.992009  2.669929      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# synthetic classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=10, random_state=7)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "## convert to dataframe\n",
    "train = pd.DataFrame(X)\n",
    "train['class']=pd.Series(y)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Missing Value Ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.0\n",
       "1        0.0\n",
       "2        0.0\n",
       "3        0.0\n",
       "4        0.0\n",
       "5        0.0\n",
       "6        0.0\n",
       "7        0.0\n",
       "8        0.0\n",
       "9        0.0\n",
       "10       0.0\n",
       "11       0.0\n",
       "12       0.0\n",
       "13       0.0\n",
       "14       0.0\n",
       "15       0.0\n",
       "16       0.0\n",
       "17       0.0\n",
       "18       0.0\n",
       "19       0.0\n",
       "class    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the percentage of missing values in each variable\n",
    "train.isnull().sum()/len(train)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the percentage of missing values in each variable\n",
    "train.isnull().sum()/len(train)*100\n",
    "# saving missing values in a variable\n",
    "a = train.isnull().sum()/len(train)*100\n",
    "# saving column names in a variable\n",
    "variables = train.columns\n",
    "variable = [ ]\n",
    "for i in range(0,12):\n",
    "    if a[i]<=20:   #setting the threshold as 20%\n",
    "        variable.append(variables[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Low Variance Filter\n",
    "\n",
    "The variables with a low variance will not affect the target variable. So, we need to calculate the variance of each variable we are given. Then drop the variables having low variance as compared to other variables in our dataset.\n",
    "Dataframe supports ```.var()``` methods for such purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         4.905241\n",
       "1         9.941251\n",
       "2         4.348611\n",
       "3        19.574028\n",
       "4         7.560405\n",
       "5        16.087825\n",
       "6        10.791600\n",
       "7         4.259212\n",
       "8        13.435237\n",
       "9         4.473907\n",
       "10        3.625765\n",
       "11       13.805811\n",
       "12        4.064385\n",
       "13        4.379133\n",
       "14       15.349630\n",
       "15       20.581083\n",
       "16        5.095812\n",
       "17        9.197114\n",
       "18        3.272266\n",
       "19        4.545181\n",
       "class     0.250225\n",
       "dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = train[['class']]\n",
    "var = numeric.var()\n",
    "numeric = numeric.columns\n",
    "variable = [ ]\n",
    "for i in range(0,len(var)):\n",
    "    if var[i]>=10:   #setting the threshold as 10%\n",
    "       variable.append(numeric[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 High Correlation filter\n",
    "High correlation between two variables means they have similar trends and are likely to carry similar information. This can bring down the performance of some models drastically (linear and logistic regression models, for instance). \n",
    "We can use ```.corr()``` method in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.291479</td>\n",
       "      <td>0.044241</td>\n",
       "      <td>-0.051087</td>\n",
       "      <td>-0.566282</td>\n",
       "      <td>-0.034016</td>\n",
       "      <td>0.046873</td>\n",
       "      <td>-0.015271</td>\n",
       "      <td>0.596412</td>\n",
       "      <td>0.273614</td>\n",
       "      <td>-0.395720</td>\n",
       "      <td>0.272480</td>\n",
       "      <td>0.225309</td>\n",
       "      <td>-0.067886</td>\n",
       "      <td>0.721090</td>\n",
       "      <td>-0.116175</td>\n",
       "      <td>-0.221489</td>\n",
       "      <td>-0.011673</td>\n",
       "      <td>0.205774</td>\n",
       "      <td>-0.055261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.291479</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700818</td>\n",
       "      <td>0.316275</td>\n",
       "      <td>0.536765</td>\n",
       "      <td>-0.319397</td>\n",
       "      <td>-0.580430</td>\n",
       "      <td>0.709933</td>\n",
       "      <td>-0.096363</td>\n",
       "      <td>-0.199641</td>\n",
       "      <td>-0.209591</td>\n",
       "      <td>0.160872</td>\n",
       "      <td>0.223277</td>\n",
       "      <td>-0.254947</td>\n",
       "      <td>-0.055736</td>\n",
       "      <td>-0.120866</td>\n",
       "      <td>0.134904</td>\n",
       "      <td>-0.337302</td>\n",
       "      <td>0.291414</td>\n",
       "      <td>-0.020054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.044241</td>\n",
       "      <td>0.700818</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.299172</td>\n",
       "      <td>0.253437</td>\n",
       "      <td>0.204966</td>\n",
       "      <td>-0.396191</td>\n",
       "      <td>0.401770</td>\n",
       "      <td>0.231982</td>\n",
       "      <td>0.081325</td>\n",
       "      <td>-0.135360</td>\n",
       "      <td>0.150301</td>\n",
       "      <td>0.040856</td>\n",
       "      <td>0.124880</td>\n",
       "      <td>-0.132545</td>\n",
       "      <td>0.135038</td>\n",
       "      <td>0.118302</td>\n",
       "      <td>-0.050234</td>\n",
       "      <td>0.168349</td>\n",
       "      <td>0.243560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.051087</td>\n",
       "      <td>0.316275</td>\n",
       "      <td>-0.299172</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018517</td>\n",
       "      <td>-0.651777</td>\n",
       "      <td>-0.282203</td>\n",
       "      <td>0.352717</td>\n",
       "      <td>-0.186072</td>\n",
       "      <td>0.029574</td>\n",
       "      <td>-0.012260</td>\n",
       "      <td>0.395806</td>\n",
       "      <td>0.357270</td>\n",
       "      <td>-0.665413</td>\n",
       "      <td>0.357786</td>\n",
       "      <td>-0.611683</td>\n",
       "      <td>0.043395</td>\n",
       "      <td>-0.342383</td>\n",
       "      <td>0.192933</td>\n",
       "      <td>-0.658378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.566282</td>\n",
       "      <td>0.536765</td>\n",
       "      <td>0.253437</td>\n",
       "      <td>0.018517</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.226579</td>\n",
       "      <td>-0.493181</td>\n",
       "      <td>0.380213</td>\n",
       "      <td>-0.290017</td>\n",
       "      <td>-0.164106</td>\n",
       "      <td>-0.185180</td>\n",
       "      <td>-0.002842</td>\n",
       "      <td>-0.263917</td>\n",
       "      <td>-0.043315</td>\n",
       "      <td>-0.334748</td>\n",
       "      <td>0.013682</td>\n",
       "      <td>0.560808</td>\n",
       "      <td>-0.362134</td>\n",
       "      <td>0.388384</td>\n",
       "      <td>0.083105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.034016</td>\n",
       "      <td>-0.319397</td>\n",
       "      <td>0.204966</td>\n",
       "      <td>-0.651777</td>\n",
       "      <td>-0.226579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.152103</td>\n",
       "      <td>-0.468021</td>\n",
       "      <td>-0.188736</td>\n",
       "      <td>0.043612</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.106410</td>\n",
       "      <td>0.100113</td>\n",
       "      <td>0.718546</td>\n",
       "      <td>-0.488488</td>\n",
       "      <td>0.405869</td>\n",
       "      <td>0.239796</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.481145</td>\n",
       "      <td>0.444578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.046873</td>\n",
       "      <td>-0.580430</td>\n",
       "      <td>-0.396191</td>\n",
       "      <td>-0.282203</td>\n",
       "      <td>-0.493181</td>\n",
       "      <td>0.152103</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.488995</td>\n",
       "      <td>0.452699</td>\n",
       "      <td>-0.132957</td>\n",
       "      <td>0.312210</td>\n",
       "      <td>-0.467180</td>\n",
       "      <td>-0.348349</td>\n",
       "      <td>0.359999</td>\n",
       "      <td>-0.190696</td>\n",
       "      <td>0.366693</td>\n",
       "      <td>-0.689512</td>\n",
       "      <td>0.687957</td>\n",
       "      <td>0.083723</td>\n",
       "      <td>0.081989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.015271</td>\n",
       "      <td>0.709933</td>\n",
       "      <td>0.401770</td>\n",
       "      <td>0.352717</td>\n",
       "      <td>0.380213</td>\n",
       "      <td>-0.468021</td>\n",
       "      <td>-0.488995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.173421</td>\n",
       "      <td>-0.199139</td>\n",
       "      <td>-0.229193</td>\n",
       "      <td>-0.179198</td>\n",
       "      <td>0.121964</td>\n",
       "      <td>-0.179085</td>\n",
       "      <td>0.029033</td>\n",
       "      <td>0.051244</td>\n",
       "      <td>-0.016624</td>\n",
       "      <td>-0.505679</td>\n",
       "      <td>0.272616</td>\n",
       "      <td>0.083479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.596412</td>\n",
       "      <td>-0.096363</td>\n",
       "      <td>0.231982</td>\n",
       "      <td>-0.186072</td>\n",
       "      <td>-0.290017</td>\n",
       "      <td>-0.188736</td>\n",
       "      <td>0.452699</td>\n",
       "      <td>0.173421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.233156</td>\n",
       "      <td>-0.216075</td>\n",
       "      <td>-0.228520</td>\n",
       "      <td>-0.327080</td>\n",
       "      <td>0.099462</td>\n",
       "      <td>0.251356</td>\n",
       "      <td>0.153699</td>\n",
       "      <td>-0.548766</td>\n",
       "      <td>0.407846</td>\n",
       "      <td>0.583858</td>\n",
       "      <td>-0.005921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.273614</td>\n",
       "      <td>-0.199641</td>\n",
       "      <td>0.081325</td>\n",
       "      <td>0.029574</td>\n",
       "      <td>-0.164106</td>\n",
       "      <td>0.043612</td>\n",
       "      <td>-0.132957</td>\n",
       "      <td>-0.199139</td>\n",
       "      <td>0.233156</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.112404</td>\n",
       "      <td>0.330993</td>\n",
       "      <td>0.060027</td>\n",
       "      <td>-0.215055</td>\n",
       "      <td>0.349592</td>\n",
       "      <td>-0.612740</td>\n",
       "      <td>0.118137</td>\n",
       "      <td>0.104992</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>-0.317078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.395720</td>\n",
       "      <td>-0.209591</td>\n",
       "      <td>-0.135360</td>\n",
       "      <td>-0.012260</td>\n",
       "      <td>-0.185180</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.312210</td>\n",
       "      <td>-0.229193</td>\n",
       "      <td>-0.216075</td>\n",
       "      <td>-0.112404</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.016075</td>\n",
       "      <td>-0.197700</td>\n",
       "      <td>0.241330</td>\n",
       "      <td>-0.706768</td>\n",
       "      <td>0.401794</td>\n",
       "      <td>0.140639</td>\n",
       "      <td>-0.020463</td>\n",
       "      <td>-0.377851</td>\n",
       "      <td>0.147123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.272480</td>\n",
       "      <td>0.160872</td>\n",
       "      <td>0.150301</td>\n",
       "      <td>0.395806</td>\n",
       "      <td>-0.002842</td>\n",
       "      <td>0.106410</td>\n",
       "      <td>-0.467180</td>\n",
       "      <td>-0.179198</td>\n",
       "      <td>-0.228520</td>\n",
       "      <td>0.330993</td>\n",
       "      <td>0.016075</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.430446</td>\n",
       "      <td>-0.323818</td>\n",
       "      <td>0.355632</td>\n",
       "      <td>-0.549610</td>\n",
       "      <td>0.570552</td>\n",
       "      <td>-0.376623</td>\n",
       "      <td>0.141091</td>\n",
       "      <td>-0.430215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.225309</td>\n",
       "      <td>0.223277</td>\n",
       "      <td>0.040856</td>\n",
       "      <td>0.357270</td>\n",
       "      <td>-0.263917</td>\n",
       "      <td>0.100113</td>\n",
       "      <td>-0.348349</td>\n",
       "      <td>0.121964</td>\n",
       "      <td>-0.327080</td>\n",
       "      <td>0.060027</td>\n",
       "      <td>-0.197700</td>\n",
       "      <td>0.430446</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.016871</td>\n",
       "      <td>0.449854</td>\n",
       "      <td>-0.468575</td>\n",
       "      <td>-0.029954</td>\n",
       "      <td>-0.477103</td>\n",
       "      <td>-0.194304</td>\n",
       "      <td>-0.106646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.067886</td>\n",
       "      <td>-0.254947</td>\n",
       "      <td>0.124880</td>\n",
       "      <td>-0.665413</td>\n",
       "      <td>-0.043315</td>\n",
       "      <td>0.718546</td>\n",
       "      <td>0.359999</td>\n",
       "      <td>-0.179085</td>\n",
       "      <td>0.099462</td>\n",
       "      <td>-0.215055</td>\n",
       "      <td>0.241330</td>\n",
       "      <td>-0.323818</td>\n",
       "      <td>-0.016871</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.538434</td>\n",
       "      <td>0.503116</td>\n",
       "      <td>-0.058069</td>\n",
       "      <td>0.143826</td>\n",
       "      <td>-0.175441</td>\n",
       "      <td>0.362805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.721090</td>\n",
       "      <td>-0.055736</td>\n",
       "      <td>-0.132545</td>\n",
       "      <td>0.357786</td>\n",
       "      <td>-0.334748</td>\n",
       "      <td>-0.488488</td>\n",
       "      <td>-0.190696</td>\n",
       "      <td>0.029033</td>\n",
       "      <td>0.251356</td>\n",
       "      <td>0.349592</td>\n",
       "      <td>-0.706768</td>\n",
       "      <td>0.355632</td>\n",
       "      <td>0.449854</td>\n",
       "      <td>-0.538434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.606650</td>\n",
       "      <td>-0.220911</td>\n",
       "      <td>-0.087896</td>\n",
       "      <td>0.276835</td>\n",
       "      <td>-0.316331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.116175</td>\n",
       "      <td>-0.120866</td>\n",
       "      <td>0.135038</td>\n",
       "      <td>-0.611683</td>\n",
       "      <td>0.013682</td>\n",
       "      <td>0.405869</td>\n",
       "      <td>0.366693</td>\n",
       "      <td>0.051244</td>\n",
       "      <td>0.153699</td>\n",
       "      <td>-0.612740</td>\n",
       "      <td>0.401794</td>\n",
       "      <td>-0.549610</td>\n",
       "      <td>-0.468575</td>\n",
       "      <td>0.503116</td>\n",
       "      <td>-0.606650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.139324</td>\n",
       "      <td>0.066193</td>\n",
       "      <td>-0.162844</td>\n",
       "      <td>0.762816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.221489</td>\n",
       "      <td>0.134904</td>\n",
       "      <td>0.118302</td>\n",
       "      <td>0.043395</td>\n",
       "      <td>0.560808</td>\n",
       "      <td>0.239796</td>\n",
       "      <td>-0.689512</td>\n",
       "      <td>-0.016624</td>\n",
       "      <td>-0.548766</td>\n",
       "      <td>0.118137</td>\n",
       "      <td>0.140639</td>\n",
       "      <td>0.570552</td>\n",
       "      <td>-0.029954</td>\n",
       "      <td>-0.058069</td>\n",
       "      <td>-0.220911</td>\n",
       "      <td>-0.139324</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.552014</td>\n",
       "      <td>-0.102629</td>\n",
       "      <td>-0.081732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.011673</td>\n",
       "      <td>-0.337302</td>\n",
       "      <td>-0.050234</td>\n",
       "      <td>-0.342383</td>\n",
       "      <td>-0.362134</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.687957</td>\n",
       "      <td>-0.505679</td>\n",
       "      <td>0.407846</td>\n",
       "      <td>0.104992</td>\n",
       "      <td>-0.020463</td>\n",
       "      <td>-0.376623</td>\n",
       "      <td>-0.477103</td>\n",
       "      <td>0.143826</td>\n",
       "      <td>-0.087896</td>\n",
       "      <td>0.066193</td>\n",
       "      <td>-0.552014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.008286</td>\n",
       "      <td>-0.222756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.205774</td>\n",
       "      <td>0.291414</td>\n",
       "      <td>0.168349</td>\n",
       "      <td>0.192933</td>\n",
       "      <td>0.388384</td>\n",
       "      <td>-0.481145</td>\n",
       "      <td>0.083723</td>\n",
       "      <td>0.272616</td>\n",
       "      <td>0.583858</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>-0.377851</td>\n",
       "      <td>0.141091</td>\n",
       "      <td>-0.194304</td>\n",
       "      <td>-0.175441</td>\n",
       "      <td>0.276835</td>\n",
       "      <td>-0.162844</td>\n",
       "      <td>-0.102629</td>\n",
       "      <td>-0.008286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.173527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.055261</td>\n",
       "      <td>-0.020054</td>\n",
       "      <td>0.243560</td>\n",
       "      <td>-0.658378</td>\n",
       "      <td>0.083105</td>\n",
       "      <td>0.444578</td>\n",
       "      <td>0.081989</td>\n",
       "      <td>0.083479</td>\n",
       "      <td>-0.005921</td>\n",
       "      <td>-0.317078</td>\n",
       "      <td>0.147123</td>\n",
       "      <td>-0.430215</td>\n",
       "      <td>-0.106646</td>\n",
       "      <td>0.362805</td>\n",
       "      <td>-0.316331</td>\n",
       "      <td>0.762816</td>\n",
       "      <td>-0.081732</td>\n",
       "      <td>-0.222756</td>\n",
       "      <td>-0.173527</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   1.000000 -0.291479  0.044241 -0.051087 -0.566282 -0.034016  0.046873   \n",
       "1  -0.291479  1.000000  0.700818  0.316275  0.536765 -0.319397 -0.580430   \n",
       "2   0.044241  0.700818  1.000000 -0.299172  0.253437  0.204966 -0.396191   \n",
       "3  -0.051087  0.316275 -0.299172  1.000000  0.018517 -0.651777 -0.282203   \n",
       "4  -0.566282  0.536765  0.253437  0.018517  1.000000 -0.226579 -0.493181   \n",
       "5  -0.034016 -0.319397  0.204966 -0.651777 -0.226579  1.000000  0.152103   \n",
       "6   0.046873 -0.580430 -0.396191 -0.282203 -0.493181  0.152103  1.000000   \n",
       "7  -0.015271  0.709933  0.401770  0.352717  0.380213 -0.468021 -0.488995   \n",
       "8   0.596412 -0.096363  0.231982 -0.186072 -0.290017 -0.188736  0.452699   \n",
       "9   0.273614 -0.199641  0.081325  0.029574 -0.164106  0.043612 -0.132957   \n",
       "10 -0.395720 -0.209591 -0.135360 -0.012260 -0.185180  0.527821  0.312210   \n",
       "11  0.272480  0.160872  0.150301  0.395806 -0.002842  0.106410 -0.467180   \n",
       "12  0.225309  0.223277  0.040856  0.357270 -0.263917  0.100113 -0.348349   \n",
       "13 -0.067886 -0.254947  0.124880 -0.665413 -0.043315  0.718546  0.359999   \n",
       "14  0.721090 -0.055736 -0.132545  0.357786 -0.334748 -0.488488 -0.190696   \n",
       "15 -0.116175 -0.120866  0.135038 -0.611683  0.013682  0.405869  0.366693   \n",
       "16 -0.221489  0.134904  0.118302  0.043395  0.560808  0.239796 -0.689512   \n",
       "17 -0.011673 -0.337302 -0.050234 -0.342383 -0.362134  0.000025  0.687957   \n",
       "18  0.205774  0.291414  0.168349  0.192933  0.388384 -0.481145  0.083723   \n",
       "19 -0.055261 -0.020054  0.243560 -0.658378  0.083105  0.444578  0.081989   \n",
       "\n",
       "          7         8         9         10        11        12        13  \\\n",
       "0  -0.015271  0.596412  0.273614 -0.395720  0.272480  0.225309 -0.067886   \n",
       "1   0.709933 -0.096363 -0.199641 -0.209591  0.160872  0.223277 -0.254947   \n",
       "2   0.401770  0.231982  0.081325 -0.135360  0.150301  0.040856  0.124880   \n",
       "3   0.352717 -0.186072  0.029574 -0.012260  0.395806  0.357270 -0.665413   \n",
       "4   0.380213 -0.290017 -0.164106 -0.185180 -0.002842 -0.263917 -0.043315   \n",
       "5  -0.468021 -0.188736  0.043612  0.527821  0.106410  0.100113  0.718546   \n",
       "6  -0.488995  0.452699 -0.132957  0.312210 -0.467180 -0.348349  0.359999   \n",
       "7   1.000000  0.173421 -0.199139 -0.229193 -0.179198  0.121964 -0.179085   \n",
       "8   0.173421  1.000000  0.233156 -0.216075 -0.228520 -0.327080  0.099462   \n",
       "9  -0.199139  0.233156  1.000000 -0.112404  0.330993  0.060027 -0.215055   \n",
       "10 -0.229193 -0.216075 -0.112404  1.000000  0.016075 -0.197700  0.241330   \n",
       "11 -0.179198 -0.228520  0.330993  0.016075  1.000000  0.430446 -0.323818   \n",
       "12  0.121964 -0.327080  0.060027 -0.197700  0.430446  1.000000 -0.016871   \n",
       "13 -0.179085  0.099462 -0.215055  0.241330 -0.323818 -0.016871  1.000000   \n",
       "14  0.029033  0.251356  0.349592 -0.706768  0.355632  0.449854 -0.538434   \n",
       "15  0.051244  0.153699 -0.612740  0.401794 -0.549610 -0.468575  0.503116   \n",
       "16 -0.016624 -0.548766  0.118137  0.140639  0.570552 -0.029954 -0.058069   \n",
       "17 -0.505679  0.407846  0.104992 -0.020463 -0.376623 -0.477103  0.143826   \n",
       "18  0.272616  0.583858  0.109800 -0.377851  0.141091 -0.194304 -0.175441   \n",
       "19  0.083479 -0.005921 -0.317078  0.147123 -0.430215 -0.106646  0.362805   \n",
       "\n",
       "          14        15        16        17        18        19  \n",
       "0   0.721090 -0.116175 -0.221489 -0.011673  0.205774 -0.055261  \n",
       "1  -0.055736 -0.120866  0.134904 -0.337302  0.291414 -0.020054  \n",
       "2  -0.132545  0.135038  0.118302 -0.050234  0.168349  0.243560  \n",
       "3   0.357786 -0.611683  0.043395 -0.342383  0.192933 -0.658378  \n",
       "4  -0.334748  0.013682  0.560808 -0.362134  0.388384  0.083105  \n",
       "5  -0.488488  0.405869  0.239796  0.000025 -0.481145  0.444578  \n",
       "6  -0.190696  0.366693 -0.689512  0.687957  0.083723  0.081989  \n",
       "7   0.029033  0.051244 -0.016624 -0.505679  0.272616  0.083479  \n",
       "8   0.251356  0.153699 -0.548766  0.407846  0.583858 -0.005921  \n",
       "9   0.349592 -0.612740  0.118137  0.104992  0.109800 -0.317078  \n",
       "10 -0.706768  0.401794  0.140639 -0.020463 -0.377851  0.147123  \n",
       "11  0.355632 -0.549610  0.570552 -0.376623  0.141091 -0.430215  \n",
       "12  0.449854 -0.468575 -0.029954 -0.477103 -0.194304 -0.106646  \n",
       "13 -0.538434  0.503116 -0.058069  0.143826 -0.175441  0.362805  \n",
       "14  1.000000 -0.606650 -0.220911 -0.087896  0.276835 -0.316331  \n",
       "15 -0.606650  1.000000 -0.139324  0.066193 -0.162844  0.762816  \n",
       "16 -0.220911 -0.139324  1.000000 -0.552014 -0.102629 -0.081732  \n",
       "17 -0.087896  0.066193 -0.552014  1.000000 -0.008286 -0.222756  \n",
       "18  0.276835 -0.162844 -0.102629 -0.008286  1.000000 -0.173527  \n",
       "19 -0.316331  0.762816 -0.081732 -0.222756 -0.173527  1.000000  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=train.drop('class', 1)\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Random Forest\n",
    "Random Forest is one of the most widely used algorithms for feature selection. It comes packaged with in-built feature importance so you donâ€™t need to program that separately. This helps us select a smaller subset of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(random_state=1, max_depth=10)\n",
    "df=pd.get_dummies(df)\n",
    "model.fit(df,train['class'])\n",
    "features = df.columns\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[-9:]  # top 10 features\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.824 (0.034)\n"
     ]
    }
   ],
   "source": [
    "# evaluate logistic regression model on raw data\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=10, random_state=7)\n",
    "# define the model\n",
    "model = LogisticRegression()\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Principal Component Analysis\n",
    "Principal Component Analysis, or PCA, might be the most popular technique for dimensionality reduction with dense data (few zero values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate pca with logistic regression algorithm for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=10, random_state=7)\n",
    "# define the pipeline\n",
    "steps = [('pca', PCA(n_components=10)), ('m', LogisticRegression())]\n",
    "model = Pipeline(steps=steps)\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Locally Linear Embedding\n",
    "Locally Linear Embedding, or LLE, creates an embedding of the dataset and attempts to preserve the relationships between neighborhoods in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://thetalog.com/images/machine-learning/locally-linear-embedding/manifoldlearning_thetalog.png \"Title\")\n",
    "![alt text](https://thetalog.com/images/machine-learning/locally-linear-embedding/lle_viz.gif \"Title\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.886 (0.028)\n"
     ]
    }
   ],
   "source": [
    "# evaluate lle and logistic regression for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=10, random_state=7)\n",
    "# define the pipeline\n",
    "steps = [('lle', LocallyLinearEmbedding(n_components=10)), ('m', LogisticRegression())]\n",
    "model = Pipeline(steps=steps)\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. PCA Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Principal Component Analysis\n",
    "\n",
    "Principal component analysis (PCA) is a statistical procedure that uses an orthogonal linear transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.\n",
    "\n",
    "This transformation is defined in such a way that the first principal component has the largest possible variance, and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.\n",
    "\n",
    "![alt text](https://lh6.googleusercontent.com/EQCguiagMeAptcwEhi1RP49WjLaWVR9s6aihUVzHUlYVpYyJK1EoTD4DbVjV77TpUM6gHe0pXeB6wyt9732_KKGtWMP6m_WHuphClravQ7LssDx_3ANyOu8VpiOfiIofdGBI5T0 \"Title\")\n",
    "\n",
    "![alt text](https://lh3.googleusercontent.com/IlPla42NUww_Fbbv5zjFjtUDD9sxhik2px7X56VW8kdmdY8jZWz1YR44pp-CrsHErHzoUpMODb8h-UCIysHOSEaq0T3ybeDG3LNu6t-Ecn1W9CqhUWXJLfvnkKH_KdMPLYhf8kU \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.2 PCA Algorithm\n",
    "\n",
    "<img src=\"http://xijun-album.oss-cn-hangzhou.aliyuncs.com/20190316Reunderstanding_PCA/Procedure%20of%20PCA.png\" alt=\"Drawing\" align=\"left\" style=\"width: 50%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Load Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# First Load the dataset to a matrix:\n",
    "\n",
    "iris = load_iris()\n",
    "dataset = np.array(iris.data)\n",
    "dataset[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Calculate the covariance matrix:\n",
    "\n",
    "Variance is the expectation of the squared deviation of a random variable from its mean. Informally, it measures the spread of a set of numbers from their mean. The mathematical definition is:\n",
    "\n",
    "![](https://miro.medium.com/max/321/1*W3v_zA6XngcZ63TXAIZEiw.png)\n",
    "\n",
    "Covariance is a measure of the joint variability of two random variables. In other words, how any 2 features vary from each other. Using the covariance is very common when looking for patterns in data. The mathematical definition is:\n",
    "\n",
    "![](https://miro.medium.com/max/617/1*iIcmw9AR70_ZzbsHfZvL6w.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <td>0.685694</td>\n",
       "      <td>-0.042434</td>\n",
       "      <td>1.274315</td>\n",
       "      <td>0.516271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <td>-0.042434</td>\n",
       "      <td>0.189979</td>\n",
       "      <td>-0.329656</td>\n",
       "      <td>-0.121639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petal length (cm)</th>\n",
       "      <td>1.274315</td>\n",
       "      <td>-0.329656</td>\n",
       "      <td>3.116278</td>\n",
       "      <td>1.295609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petal width (cm)</th>\n",
       "      <td>0.516271</td>\n",
       "      <td>-0.121639</td>\n",
       "      <td>1.295609</td>\n",
       "      <td>0.581006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "sepal length (cm)           0.685694         -0.042434           1.274315   \n",
       "sepal width (cm)           -0.042434          0.189979          -0.329656   \n",
       "petal length (cm)           1.274315         -0.329656           3.116278   \n",
       "petal width (cm)            0.516271         -0.121639           1.295609   \n",
       "\n",
       "                   petal width (cm)  \n",
       "sepal length (cm)          0.516271  \n",
       "sepal width (cm)          -0.121639  \n",
       "petal length (cm)          1.295609  \n",
       "petal width (cm)           0.581006  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariance_matrix = pd.DataFrame(data = np.cov(dataset, rowvar = False), columns = iris.feature_names, index = iris.feature_names)\n",
    "covariance_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Calculate the eigenvalues and eigenvectors:\n",
    "\n",
    "eigenvalues and eigenvectors are the basic terms of math please take a alook on(https://www.mathsisfun.com/algebra/eigenvalue.html) A simple example is that an eigenvector does not change direction in a transformation:\n",
    "\n",
    "<img src=\"https://www.mathsisfun.com/algebra/images/eigen-transform.svg\" align=\"left\" style=\"width: 30%;\"/>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a square matrix A, an Eigenvector and Eigenvalue make this equation true:\n",
    "\n",
    "![](https://www.mathsisfun.com/algebra/images/eigenvalue.svg)\n",
    "\n",
    "The eigenvector associated with the largest eigenvalue indicates the direction in which the data has the most variance. Hence, using eigenvalues we will know what eigenvectors capture the most variability in our data.\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/lNHqt.gif \" alt=\"Drawing\" align=\"left\" style=\"width: 50%;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.22824171 0.24267075 0.0782095  0.02383509]\n",
      "[[ 0.36138659 -0.65658877 -0.58202985  0.31548719]\n",
      " [-0.08452251 -0.73016143  0.59791083 -0.3197231 ]\n",
      " [ 0.85667061  0.17337266  0.07623608 -0.47983899]\n",
      " [ 0.3582892   0.07548102  0.54583143  0.75365743]]\n"
     ]
    }
   ],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "print(eigenvalues)\n",
    "print(eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4. Choose the first K eigenvalues (K principal components/axises):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "totalSum = sum(eigenvalues)\n",
    "variables_explained = [(i / totalSum) for i in sorted(eigenvalues, reverse = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9246187232017268,\n",
       " 0.05306648311706791,\n",
       " 0.017102609807929672,\n",
       " 0.005212183873275513]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36138659, -0.65658877],\n",
       "       [-0.08452251, -0.73016143],\n",
       "       [ 0.85667061,  0.17337266],\n",
       "       [ 0.3582892 ,  0.07548102]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vector = eigenvectors[:,:2]\n",
    "feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Step 5. Build the new reduced dataset:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/648/1*hhxeK-lJ9z1h0lK-hHdVaQ.png\" alt=\"Drawing\"  style=\"width: 30%;\"/>\n",
    "\n",
    "Matrix multiplication\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*1mWiBY253nTA2pwq0195Ag.png\" alt=\"Drawing\"  style=\"width: 30%;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector_transpose = np.transpose(feature_vector)\n",
    "dataset_transpose = np.transpose(dataset)\n",
    "new_dataset_transpose = np.matmul(feature_vector_transpose, datasetTranspose)\n",
    "new_dataset = np.transpose(new_dataset_transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.81823951, -5.64634982],\n",
       "       [ 2.78822345, -5.14995135],\n",
       "       [ 2.61337456, -5.18200315]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Reference \n",
    "* https://medium.com/free-code-camp/an-overview-of-principal-component-analysis-6340e3bc4073\n",
    "* https://builtin.com/data-science/step-step-explanation-principal-component-analysis\n",
    "* https://xijunlee.github.io/2019/03/10/reunderstanding-of-PCA/\n",
    "* https://medium.com/free-code-camp/an-overview-of-principal-component-analysis-6340e3bc4073\n",
    "* http://xijun-album.oss-cn-hangzhou.aliyuncs.com/20190316Reunderstanding_PCA/Procedure%20of%20PCA.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci-tools-nre",
   "language": "python",
   "name": "sci-tools-nre"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
